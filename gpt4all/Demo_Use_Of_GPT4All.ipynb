{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAhnQcTIsL3sO1PnUnWtXX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidowu1/Generative-AI-Recipes/blob/main/Demo_Use_Of_GPT4All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo Use Of GPT4All\n",
        "\n",
        "There has been a growing interest in Generative AI, especially Large Language Models (LLM) such as GPT-4 and its impact in the development of business solutions and consumer products. Recently, there has been an accessable addition to the LLM ecosystem known as GPT4All.\n",
        "\n",
        "GPT4All is an offline alternative to GPT4 alternative that can run on your computer using its CPU. GPT4All is developed by [Nomic AI](https://github.com/nomic-ai/gpt4all).\n",
        "\n",
        "A typical GPT4All model is a 3GB - 8GB file that you can download and run on your local machine. Nomic AI supports and maintains this software ecosystem to enforce quality and security alongside spearheading the effort to allow any person or enterprise to easily train and deploy their own on-edge large language models.\n",
        "\n",
        "Nomic's GPT4All model offering is currently compatible with three variants of the Transformer neural network architecture, namely:\n",
        " - LLaMa\n",
        " - GPT-J\n",
        " - MPT\n",
        "\n",
        "Full details of these variants are detailed in this [json file](https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-chat/metadata/models.json)\n",
        "\n",
        "This notebook will provde a quick guide on how to setup and use GPT4ALL to create a really simply Q & A chatbot on a local computer CPU. From my experience, typically to run GPT4All on a local computer you might need at least 16 GB of RAM.This notebook will describe the following steps:\n",
        " - Installation of GPT4ALL\n",
        " - Install Jupyter Dash to create simple interactive UI controls to demo Chatbot\n",
        " - Import the packgages\n",
        " - Using GPT4ALL to create a simple Q & A chatbot\n",
        ""
      ],
      "metadata": {
        "id": "jsX9GvfHadsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KVLkdEISglqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of GPT4All\n",
        " - I will be running this on a Google colab instance\n",
        " - If you want to install this on you local PC, I will advise you to setup a virtual environment (with python 3.9) using Conda"
      ],
      "metadata": {
        "id": "EoHFaIzUgowb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OIVThwWaGmF",
        "outputId": "2a8de700-68d0-4239-8022-920f80c51d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt4all\n",
            "  Downloading gpt4all-0.3.4-py3-none-manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Jupyter Dash to create simple interactive UI controls to demo Chatbot"
      ],
      "metadata": {
        "id": "GamhiPN33drA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jupyter-dash pip install \"dash-bootstrap-components<1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbTHIMX_hi3t",
        "outputId": "c2cee39e-a3b5-4c41-b897-157a1ed77d54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/197.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define imports"
      ],
      "metadata": {
        "id": "ocN4aDGo380U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt4all\n",
        "import time\n",
        "import dash\n",
        "import dash_html_components as html\n",
        "import dash_core_components as dcc\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash.dependencies import Input, Output, State\n",
        "from jupyter_dash import JupyterDash\n",
        "import pandas as pd\n",
        "import json\n",
        "from enum import Enum, auto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "yKKZ_g9r3rOB",
        "outputId": "4ed86cda-31cb-429d-e0cb-cf7e86d22d5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-bebd47436a2e>:4: UserWarning: \n",
            "The dash_html_components package is deprecated. Please replace\n",
            "`import dash_html_components as html` with `from dash import html`\n",
            "  import dash_html_components as html\n",
            "<ipython-input-3-bebd47436a2e>:5: UserWarning: \n",
            "The dash_core_components package is deprecated. Please replace\n",
            "`import dash_core_components as dcc` with `from dash import dcc`\n",
            "  import dash_core_components as dcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global constants and variables"
      ],
      "metadata": {
        "id": "KfsKG_1tB--e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat bot model global variable\n",
        "chatbot = None\n",
        "\n",
        "# Chat delimiter\n",
        "DELIMITER = \":\"\n",
        "\n",
        "# Line divider for console output\n",
        "LINE_DIVIDER = \"==========\" * 5\n",
        "\n",
        "\n",
        "class gpt4all_model_type(Enum):\n",
        "  gpt_j = auto()\n",
        "  llama = auto()\n",
        "  mpt = auto()\n",
        "\n",
        "GPT4ALL_MODELS = {\n",
        "    gpt4all_model_type.gpt_j: \"ggml-gpt4all-j-v1.3-groovy.bin\",\n",
        "    gpt4all_model_type.llama: \"GPT4All-13B-snoozy.ggmlv3.q4_0.bin\",\n",
        "    gpt4all_model_type.mpt: \"ggml-mpt-7b-base.bin\",\n",
        "}"
      ],
      "metadata": {
        "id": "txT1_QGECHWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Chat bot based on GPT4All Q & A inferencing"
      ],
      "metadata": {
        "id": "EgZ18htsAq96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT4AllChatBot(object):\n",
        "  \"\"\"\n",
        "  GPT4All Chat Bot\n",
        "  \"\"\"\n",
        "  def __init__(self, model_type: gpt4all_model_type=gpt4all_model_type.gpt_j):\n",
        "    \"\"\"\n",
        "    Constructor\n",
        "    :param model_type: Model type\n",
        "    \"\"\"\n",
        "    print(\"This is a simple Chat Bot using GPT4All inferencing...\")\n",
        "    self._model_type = GPT4ALL_MODELS[model_type]\n",
        "    self._model = gpt4all.GPT4All(self._model_type)\n",
        "\n",
        "  def _run(self, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Runs the Q & A inferencing\n",
        "    :param question: Prompt/question\n",
        "    :return: Response/answer\n",
        "    \"\"\"\n",
        "    try:\n",
        "      request = [{\"role\": \"user\", \"content\": question}]\n",
        "      answer = self._model.generate(question, streaming=False)\n",
        "      response = json.dumps(answer, indent=4)\n",
        "    except Exception as ex:\n",
        "      error_msg = f\"{str(ex) } Error has occurred\\n\"\n",
        "      raise Exception(error_msg)\n",
        "    return response\n",
        "\n",
        "  def computeChatResponse(self, prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Computes the chatbot response\n",
        "    :param question: Prompt/question\n",
        "    :return: Response/answer\n",
        "    \"\"\"\n",
        "    response = self._run(prompt).replace('\"', '').replace('\\\\n', '')\n",
        "    response = f\"{prompt}: {response}\"\n",
        "    return response\n",
        "\n"
      ],
      "metadata": {
        "id": "Uv4xY_-hA7vr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the Chat Bot NLP Model"
      ],
      "metadata": {
        "id": "WrCX_3JKDqoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if chatbot == None:\n",
        "  chatbot = GPT4AllChatBot(model_type=gpt4all_model_type.gpt_j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te3nlciMDwQz",
        "outputId": "57283728-46e9-4eb1-fd36-dbc7da53724d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a simple Chat Bot using GPT4All inferencing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.79G/3.79G [01:25<00:00, 44.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded at:  /root/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test it on a simple Q & A task:"
      ],
      "metadata": {
        "id": "6GqZ8ngls84M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testQAndAInferencing():\n",
        "  \"\"\"\n",
        "  Invokes the Q and A inferencing tests\n",
        "  \"\"\"\n",
        "  LINE_DIVIDER = \"==========\" * 5\n",
        "  prompts = [\n",
        "      \"What is 12 + 13?\",\n",
        "      \"What is the square root of 144?\",\n",
        "      \"What is reinforcement Learning?\",\n",
        "      \"What is semi-supervised Learning?\",\n",
        "      \"Who was the president US from 2008 to 2012?\",\n",
        "      \"Which team won the English premier league in 2012?\",\n",
        "      ]\n",
        "  for prompt in prompts:\n",
        "    start_time = time.time()\n",
        "    response = chatbot.computeChatResponse(prompt)\n",
        "    elasped_time = round(time.time() - start_time, 2)\n",
        "    print(f\"Question: {prompt}\\tAnswer:{response}\")\n",
        "    print(f\"Inference time: {elasped_time}\")\n",
        "    print(f\"{LINE_DIVIDER}\\n\\n\")\n",
        "\n",
        "testQAndAInferencing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6-pBOIKtC3C",
        "outputId": "fc954874-d4b9-46ef-8f05-d2b8b8579018"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is 12 + 13?\tAnswer:What is 12 + 13?: 12 plus 13 equals 25.\n",
            "Inference time: 17.65\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "Question: What is the square root of 144?\tAnswer:What is the square root of 144?: The square root of 144 is 12.\n",
            "Inference time: 7.22\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "Question: What is reinforcement Learning?\tAnswer:What is reinforcement Learning?: Reinforcement learning (RL) is a type of machine learning algorithm that involves training an agent to make decisions in a complex environment. The goal is to maximize the cumulative reward over time, which can be achieved by learning from the environment and making decisions based on that experience. The agent receives a reward for each action it takes, and the environment provides feedback in the form of rewards or penalties. The agent then adjusts its behavior based on the feedback to maximize cumulative reward over time.\n",
            "Inference time: 48.97\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "Question: What is semi-supervised Learning?\tAnswer:What is semi-supervised Learning?: Semi-supervised learning is a type of machine learning where the algorithm uses both labeled and unlabeled data to train a model. The labeled data is used for training the algorithm, while the unlabeled data is used to improve its performance. The goal of semi-supervised learning is to balance the amount of labeled and unlabeled data used in training, which can improve the accuracy of predictions.\n",
            "Inference time: 47.78\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "Question: Who was the president US from 2008 to 2012?\tAnswer:Who was the president US from 2008 to 2012?: The President of the United States from 2008 to 2012 was Barack Obama.\n",
            "Inference time: 12.83\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "Question: Which team won the English premier league in 2012?\tAnswer:Which team won the English premier league in 2012?: The team that won the English Premier League in 2012 was Manchester City.\n",
            "Inference time: 12.89\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5Y-UoUsuLAE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create interactive UI controls for the Chat Bot"
      ],
      "metadata": {
        "id": "dcbzoCbI-2xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create chat bot text box control"
      ],
      "metadata": {
        "id": "aMOvN5Ky_MWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DELIMITER = \":\"\n",
        "\n",
        "def textbox(text, box=\"other\"):\n",
        "    style = {\n",
        "        \"max-width\": \"55%\",\n",
        "        \"width\": \"max-content\",\n",
        "        \"padding\": \"10px 15px\",\n",
        "        \"border-radius\": \"25px\",\n",
        "    }\n",
        "\n",
        "    if box == \"self\":\n",
        "        style[\"margin-left\"] = \"auto\"\n",
        "        style[\"margin-right\"] = 0\n",
        "\n",
        "        color = \"primary\"\n",
        "        inverse = True\n",
        "\n",
        "    elif box == \"other\":\n",
        "        style[\"margin-left\"] = 0\n",
        "        style[\"margin-right\"] = \"auto\"\n",
        "\n",
        "        color = \"light\"\n",
        "        inverse = False\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect option for `box`.\")\n",
        "\n",
        "    return dbc.Card(text, style=style, body=True, color=color, inverse=inverse)\n"
      ],
      "metadata": {
        "id": "2n9FACTe4b3R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the chat bot UI layout"
      ],
      "metadata": {
        "id": "1FHw020B_TJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = html.Div(\n",
        "    style={\n",
        "        \"width\": \"80%\",\n",
        "        \"max-width\": \"800px\",\n",
        "        \"height\": \"70vh\",\n",
        "        \"margin\": \"auto\",\n",
        "        \"overflow-y\": \"auto\",\n",
        "    },\n",
        "    id=\"display-conversation\",\n",
        ")\n",
        "\n",
        "controls = dbc.InputGroup(\n",
        "    style={\"width\": \"80%\", \"max-width\": \"800px\", \"margin\": \"auto\"},\n",
        "    children=[\n",
        "        dbc.Input(id=\"user-input\", placeholder=\"Write to the chatbot...\", type=\"text\"),\n",
        "        dbc.InputGroupAddon(dbc.Button(\"Submit\", id=\"submit\"), addon_type=\"append\",),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Define app\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "server = app.server\n",
        "\n",
        "\n",
        "# Define Layout\n",
        "app.layout = dbc.Container(\n",
        "    fluid=True,\n",
        "    children=[\n",
        "        html.H1(\"Demo of GPT4All Chatbot\"),\n",
        "        html.Hr(),\n",
        "        dcc.Store(id=\"store-conversation\", data=\"\"),\n",
        "        conversation,\n",
        "        controls,\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "p5__GGZd_yKO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define chat bot UI callback"
      ],
      "metadata": {
        "id": "skZORlxNAHmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.callback(\n",
        "    Output(\"display-conversation\", \"children\"), [Input(\"store-conversation\", \"data\")]\n",
        ")\n",
        "def updateDisplay(chat_history):\n",
        "    messages = [\n",
        "        textbox(x, box=\"self\") if i % 2 == 0 else textbox(x, box=\"other\")\n",
        "        for i, x in enumerate(chat_history.split(DELIMITER))\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    [Output(\"store-conversation\", \"data\"), Output(\"user-input\", \"value\")],\n",
        "    [Input(\"submit\", \"n_clicks\"), Input(\"user-input\", \"n_submit\")],\n",
        "    [State(\"user-input\", \"value\"), State(\"store-conversation\", \"data\")],\n",
        ")\n",
        "def runChatbot(n_clicks, n_submit, user_input, chat_history):\n",
        "    if n_clicks == 0:\n",
        "        return \"\", \"\"\n",
        "\n",
        "    if user_input is None or user_input == \"\":\n",
        "        return chat_history, \"\"\n",
        "    response = chatbot.computeChatResponse(user_input)\n",
        "    chat_history = chat_history + \":\" + response\n",
        "    return chat_history, \"\"\n"
      ],
      "metadata": {
        "id": "W8_r3GYYSwzZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the chat Bot"
      ],
      "metadata": {
        "id": "9MPIdHh_DxpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app.run_server(mode='inline', port=\"8099\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "m8cZgqLfD3Tm",
        "outputId": "0dd77df6-a821-4e94-edab-41425fabc7ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dash is running on http://127.0.0.1:8099/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:dash.dash:Dash is running on http://127.0.0.1:8099/\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8099, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ZGP-n1eHcWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "kLgeqIMAHc4x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SbsYs4tmDlRS"
      }
    }
  ]
}